apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  name: llama-cpp-python-runtime
  label:
    app: llama-cpp-python
spec:
  tolerations:
    - key: nvidia.com/gpu
      operator: Exists
      effect: NoSchedule
  multiModel: false
  supportedModelFormats:
    - autoSelect: true
      name: gguf
  containers:
    - name: kserve-container
      # image: quay.io/dtrifiro/llama-cpp-python-serving:latest # cpu only
      image: quay.io/dtrifiro/llama-cpp-python-serving:latest-cuda # GPU-accelerated
      ommands: ["python3", "-m", "llama_cpp.server"]
      env:
        - name: MODEL
          value: /mnt/models/mistral-7b-q2k-extra-small.gguf
        # - name: CONFIG_FILE
        #   value: <>
        # - name: PORT
        #   value: 8000
      # readinessProbe:
      #   # TODO
      # livenessProbe:
      #   # TODO
      ports:
        - containerPort: 8000
          protocol: TCP
      resources: # configure as required, depending on the model
        requests:
          cpu: 4
          nvidia.com/gpu: "1"
          memory: 8Gi # llama2 7b models
          # memory: 16Gi # llama2 13b models
          # memory: 64Gi # llama2 70b models
        limits:
          cpu: 8
          nvidia.com/gpu: "1"
          memory: 64Gi # llama2 7b models
          # memory: 16Gi # llama2 13b models
          # memory: 64Gi # llama2 70b models
