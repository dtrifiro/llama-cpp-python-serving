apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  name: llama-cpp-python-runtime
spec:
  multiModel: false
  supportedModelFormats:
    - autoSelect: true
      name: gguf
  containers:
    - name: kserve-container
      # image: quay.io/dtrifiro/llama-cpp-python-serving:latest # cpu only
      image: quay.io/dtrifiro/llama-cpp-python-serving:latest-cuda # GPU-accelerated
      ommands: ["python3", "-m", "llama_cpp.server"]
      env:
        - name: MODEL
          value: /mnt/models/mistral-7b-q2k-extra-small.gguf
        # - name: CONFIG_FILE
        #   value: <>
        # - name: PORT
        #   value: 8000
      # readinessProbe:
      #   # TODO
      # livenessProbe:
      #   # TODO
      ports:
        - containerPort: 8000
          protocol: TCP
      resources: # configure as required, depending on the model
        requests:
          # cpu: 8
          # "nvidia.com/gpu": 1 # uncomment if using GPU, use the latest-cuda tag
          memory: 8Gi # llama2 7b models
          # memory: 16Gi # llama2 13b models
          # memory: 64Gi # llama2 70b models
